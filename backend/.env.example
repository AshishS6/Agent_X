# Go Backend Environment Configuration

# Server
PORT=3001
GIN_MODE=debug

# Database
DATABASE_URL=postgres://postgres:postgres@localhost:5432/agent_x?sslmode=disable

# Concurrency Limits
GLOBAL_CONCURRENCY_LIMIT=10
DEFAULT_TOOL_CONCURRENCY_LIMIT=5

# ============================================
# LLM Router Configuration (AUTHORITATIVE)
# ============================================
# All LLM calls go through the centralized router
# Router implements local-first strategy with cloud fallback

# Routing Mode
LLM_MODE=local_first
# Options: local_first (try local, fallback to cloud)
#          cloud_only (only use cloud providers)
#          local_only (only use local providers)

# Provider Priority (comma-separated, tried in order)
LLM_PRIORITY=ollama,openai,anthropic

# Fallback Behavior
LLM_FALLBACK_ENABLED=true

# Default Models
LLM_LOCAL_MODEL=qwen2.5:7b-instruct
LLM_CLOUD_MODEL=openai:gpt-4-turbo-preview

# Budget Limits (optional, for future use)
# LLM_DAILY_BUDGET_USD=5.00
# LLM_AGENT_BUDGET_blog_agent=2.00

# Provider Configuration
OLLAMA_BASE_URL=http://localhost:11434
OPENAI_API_KEY=
ANTHROPIC_API_KEY=

# Legacy Support (for backward compatibility)
# LLM_PROVIDER is still read but router takes precedence
LLM_PROVIDER=openai

# CORS
CORS_ORIGINS=http://localhost:5173,http://localhost:3000

# Logging
LOG_LEVEL=info
